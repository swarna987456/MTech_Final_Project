{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bf794a3-62d1-4227-9da2-898296709af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade googletrans==4.0.0-rc1\n",
    "# !pip install --upgrade pyspellchecker\n",
    "# !pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "942be0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from googletrans import Translator\n",
    "from spellchecker import SpellChecker\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc519e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_informative_words = pd.read_csv('final_words_to_remove_updated.csv')\n",
    "less_informative_words = less_informative_words['words_to_remove'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d83fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2784509",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 ='login issue'\n",
    "\n",
    "text2 = '-verified user details.(employee# & manager name)_x000D_\\n-checked the user name in ad and reset the password._x000D_\\n-advised the user to login and check._x000D_\\n-caller confirmed that he was able to login._x000D_\\n-issue resolved.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69de3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.translator = Translator()\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def text_cleaning_steps_1(self, short_text, long_text):\n",
    "        text = short_text + ' ' + long_text\n",
    "        text = text.lower()\n",
    "        translated = self.translator.translate(text, src='de', dest='en')\n",
    "        text = translated.text\n",
    "        text = text.replace('x000d', ' ')\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('â€', ' ')\n",
    "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', ' ', text)\n",
    "        text = re.sub(r'\\S+\\.\\S+@gmail\\.com', ' ', text)\n",
    "        text = re.sub(r'\\[\\s*cid:[^\\]]+\\]', ' ', text)\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        text = re.sub(r\"[^\\w\\s']\", ' ', text)\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", ' ', text)\n",
    "        text = ' '.join(text.split())\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def lemmatize_text(self, text):\n",
    "        lemmas = []\n",
    "        for token in self.nlp(text):\n",
    "            if token.text.strip():\n",
    "                lemmas.append(token.lemma_.lower())\n",
    "        return lemmas\n",
    "    \n",
    "    def remove_stopwords(self,lemmas):\n",
    "        tokens = [token for token in lemmas if token not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def removing_noise(self,tokens):\n",
    "        tokens = [token for token in tokens if len(token) > 1 and token not in less_informative_words]\n",
    "        return tokens\n",
    "    \n",
    "    def additional_cleaning(self,tokens):\n",
    "        tokens = ['abended' if token == 'evening' else token for token in tokens]\n",
    "        tokens = list(set(tokens))\n",
    "        text = ' '.join(tokens)\n",
    "        return text\n",
    "    \n",
    "    def process_text(self, short_text, long_text):\n",
    "        cleaned_text = self.text_cleaning_steps_1(short_text, long_text)\n",
    "        lemmas = self.lemmatize_text(cleaned_text)\n",
    "        tokens = self.remove_stopwords(lemmas)\n",
    "        tokens = self.removing_noise(tokens)\n",
    "        text = self.additional_cleaning(tokens)\n",
    "        \n",
    "        print('The text has been cleaned')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25e814c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has been cleaned\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'name check ad advise caller resolve issue verified login employee reset password manager confirmed user details'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = TextProcessor()\n",
    "cleaned_text = processor.process_text(text1, text2)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18620e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c90bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = len(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc09612d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f9db4cf-e524-4099-aef9-42e5dea5bc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14fa1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = joblib.load('LabelEncoder.pkl')\n",
    "scaler = joblib.load('MinMaxScaler.pkl')\n",
    "vectorize = joblib.load('TfidfVectorizer.pkl')\n",
    "model = joblib.load('OVO_LR_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfd6659e-9971-46bc-ba91-cb23cc16d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text_length, cleaned_text, scaler, vectorize, model, label_encoder):\n",
    "    test_description_dense = np.hstack((\n",
    "        vectorize.transform([cleaned_text]).toarray(),\n",
    "        scaler.transform(np.array(text_length).reshape(-1, 1))\n",
    "    ))\n",
    "    \n",
    "    pred = model.predict(test_description_dense)\n",
    "    print(pred)\n",
    "    return label_encoder.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "017076ca-10f6-41c4-bc72-ce5f12b44c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GRP_0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lable = predict_label(text_length, cleaned_text, scaler, vectorize, model, label_encoder)[0]\n",
    "predicted_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f4f4063-9a12-4d52-996c-032ae4e2c11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GRP_0', 'GRP_10', 'GRP_12', 'GRP_13', 'GRP_14', 'GRP_19', 'GRP_2',\n",
       "       'GRP_24', 'GRP_25', 'GRP_3', 'GRP_33', 'GRP_4', 'GRP_5', 'GRP_6',\n",
       "       'GRP_8', 'GRP_9'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e76534-b456-4697-86c6-cc789a469087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Mtech_projec_app)",
   "language": "python",
   "name": "it_tickets_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
